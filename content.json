{"pages":[],"posts":[{"title":"Cross Entropy, KL Divergency and MLE","text":"p: real probability, data q: predict probability 1. Min KL Divergence equals to min cross entropyCross Entropy: KL Divergence: a). Minimize KL divergence is equal to minimize cross entropy. b). Kl divergence is non-negative and it reaches 0 when , So the minimum of KL Divergence and Cross Entropy is when . 2. Min KL Divergence equals to max MLEMLE: KL: Appendix:(1). ​ Proof: ​ So x = 1 is the maximum point of f(x). f(1) = 0. So f(x) &lt;= 0. References:[1] https://stats.stackexchange.com/questions/335197/why-kl-divergence-is-non-negative [2] http://www.hongliangjie.com/2012/07/12/maximum-likelihood-as-minimize-kl-divergence/","link":"/2018/11/27/Cross-Entropy-KL-MLE/"},{"title":"Visualize High-Dimensional Data","text":"Target: Minimize the divergence between two distributions: ​ (1) a distribution that measures pairwise similarities of the input objects ​ (2) a distribution that measures pairwise similarities of the low-dimensional points in the embedding Inputs: Outputs: 1. t-sneThe similarity probability of inputs (Gaussian kernel): The similarity probability of outputs (Student-t kernel): Minimize Objective Function: 2. Large-VisFollowing graph shows a typical pipeline of visualize high-dimensional data using K-NNG. (1) Use Random Projection Trees to construct K-NNG, and also consider second-order neighbors. (2) Once K-NNG is constructed, we want the low dimension distribution to capture this relationship. (3) It uses the idea of negative sampling in Word2Vec to maximize the likelihood function: Appendix:(1). Gaussian Distribution Then when u =0, (2) Student-t Distribution Then, (3) The choose of student-t Distribution in low-dimensionIf we draw the probability density function of Gaussian distribution (mu=0, sigma=1) and student-t distribution (n=1). We can see that Student-t distribution has long tails. Let’s consider two situations: a. When two points are similar (high probability), we can see that the distance in student-t distribution is shorter than the ones in Gaussian distribution. b. When two points are irrelevant to each other (low probability), the distance in student-t distribution is much longer than the ones in Gaussian distribution. We can conclude that the use of student-t distribution in low-dimension can keep similar data close while dissimilar data far apart. References:[1] Maaten, L.V.D. and Hinton, G., 2008. Visualizing data using t-SNE. Journal of machine learning research, 9(Nov), pp.2579-2605. http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf [2] Van Der Maaten, L., 2014. Accelerating t-SNE using tree-based algorithms. The Journal of Machine Learning Research, 15(1), pp.3221-3245. http://www.jmlr.org/papers/volume15/vandermaaten14a/vandermaaten14a.pdf [3] Tang, J., Liu, J., Zhang, M. and Mei, Q., 2016, April. Visualizing large-scale and high-dimensional data. In Proceedings of the 25th International Conference on World Wide Web (pp. 287-297). International World Wide Web Conferences Steering Committee. https://arxiv.org/pdf/1602.00370.pdf","link":"/2018/11/27/Visualize-High-Dimensional-Data/"},{"title":"Word Embedding","text":"1. Word2Vec CBOW: use context to predict target word Skip-Gram: use target word to predict context Skip-GramMaximize the log likelihood (window size c): where the probability of a word appears near a given word is: The dot product of two word vectors is used to measure their similarity. The network: 2. Fasttext Task: text classification (use a sentence or document to predict its class) Minimize the negative log likelihood: where x represent the N-gram feature in this document, A is the look-up table stored the word embedding, B is also a weight matrix. The network: The network is similar to the ones of CBOW. However, the output here is the class instead of target word. Tricks:(1) Hierarchical SoftmaxIt is a method to transfer a multi-class classification problem into several binary-class classification problems. It can be used in Word2Vec and Fasttext when the number of classes to be classified is large. Use a Huffman tree. Each leaf represent a class. Calculate the probability of each class using chain rule. (2) Negative samplingAnother method to transfer a multi-class classification problem into several binary-class classification problems. In Skip-Gram, the objective function changed to: It aims at maximize the probability of context word and minimize the probability of a subset of words do not belonging to its context. The probability of a word been choose corresponds to its frequency in the corpus. References:[1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119). https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf [2] McCormick, 2016. Word2Vec Tutorial - The Skip-Gram Model http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ [3] Joulin, A., Grave, E., Bojanowski, P. and Mikolov, T., 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759. https://arxiv.org/pdf/1607.01759.pdf [4] Benjamin. 2017. Hierarchical Softmax. http://building-babylon.net/2017/08/01/hierarchical-softmax/","link":"/2018/11/27/Word-Embedding/"},{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/11/24/hello-world/"}],"tags":[],"categories":[]}