{"pages":[],"posts":[{"title":"Hello World","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new \"My New Post\" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","link":"/2018/11/24/hello-world/"},{"title":"Word Embedding","text":"1. Word2Vec CBOW: use context to predict target word Skip-Gram: use target word to predict context Skip-GramMaximize the log likelihood (window size c): where the probability of a word appears near a given word is: The dot product of two word vectors is used to measure their similarity. The network: 2. Fasttext Task: text classification (use a sentence or document to predict its class) Minimize the negative log likelihood: where x represent the N-gram feature in this document, A is the look-up table stored the word embedding, B is also a weight matrix. The network: The network is similar to the ones of CBOW. However, the output here is the class instead of target word. Tricks:(1) Hierarchical SoftmaxIt is a method to transfer a multi-class classification problem into several binary-class classification problems. It can be used in Word2Vec and Fasttext when the number of classes to be classified is large. Use a Huffman tree. Each leaf represent a class. Calculate the probability of each class using chain rule. (2) Negative samplingAnother method to transfer a multi-class classification problem into several binary-class classification problems. In Skip-Gram, the objective function changed to: It aims at maximize the probability of context word and minimize the probability of a subset of words do not belonging to its context. The probability of a word been choose corresponds to its frequency in the corpus. References:[1] Mikolov, T., Sutskever, I., Chen, K., Corrado, G.S. and Dean, J., 2013. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems (pp. 3111-3119). https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf [2] McCormick, 2016. Word2Vec Tutorial - The Skip-Gram Model http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/ [3] Joulin, A., Grave, E., Bojanowski, P. and Mikolov, T., 2016. Bag of tricks for efficient text classification. arXiv preprint arXiv:1607.01759. https://arxiv.org/pdf/1607.01759.pdf [4] Benjamin. 2017. Hierarchical Softmax. http://building-babylon.net/2017/08/01/hierarchical-softmax/","link":"/2018/11/27/Word-Embedding/"}],"tags":[],"categories":[]}